{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7252012,"sourceType":"datasetVersion","datasetId":4201810},{"sourceId":11476539,"sourceType":"datasetVersion","datasetId":7192788},{"sourceId":11484942,"sourceType":"datasetVersion","datasetId":7198381}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ¯ Use Case: AI-Powered Interview Agent\n\nHiring technical talent often involves **manually assessing** candidate answers during interviews.  \nThis process is often:\n\n- ğŸŒ **Slow**\n- ğŸ™… **Subjective**\n- ğŸ” **Inconsistent**\n\n---\n\n## ğŸ’¡ What This Notebook Does\n\nThis notebook demonstrates a **GenAI-powered Interview Agent** using a local large language model (LLM).  \nIt can:\n\nğŸ§  **1. Ask** technical questions from a curated dataset  \nğŸ’¬ **2. Accept** a candidateâ€™s answer (real or synthetic)  \nğŸ§¾ **3. Evaluate** the response with:\n\n- âœ… Strengths\n- âŒ Weaknesses\n- ğŸ“ˆ Suggestions\n- ğŸ§  Score out of 10\n\n---\n\n## âš™ï¸ Powered By\n\n- ğŸ” `google/flan-t5-large` LLM  \n- ğŸ¤— Hugging Face Transformers pipeline  \n- âœ¨ Few-shot prompting + structured feedback generation  \n- ğŸ“Š Synthetic dataset creation for scalable evaluation\n---","metadata":{}},{"cell_type":"markdown","source":"## Project Overview\n\nThis project simulates an AI-driven interview evaluation system. It leverages Generative AI models to assess candidate responses to technical interview questions and provide feedback. The process includes generating realistic candidate answers, comparing them to ideal answers, and offering personalized suggestions for improvement.\n\nKey features of the project:\n- **Feedback Generation**: Using the Flan-T5 Large model, it generates structured feedback based on the candidate's answer, suggesting areas for improvement.\n- **Semantic Similarity Matching**: A sentence transformer model calculates semantic similarity between ideal and candidate answers to provide more accurate feedback.\n- **Few-shot Prompting**: Utilizes few-shot prompting to guide the model in generating helpful and relevant feedback based on examples of previous interview evaluations.\n- **LoRA Fine-Tuning**: Fine-tunes the model with a custom LoRA adapter to enhance its performance in generating specialized feedback.\n\nThis approach offers a comprehensive solution to automate the feedback process for interview assessments and aims to make interview evaluations more efficient, consistent, and informative for both candidates and interviewers.\n","metadata":{}},{"cell_type":"markdown","source":"## GenAI Capabilities Used\n\nThe notebook leverages several GenAI capabilities:\n\n- **Text Generation** âœï¸: Uses Flan-T5 Large model to generate structured feedback on interview responses, controlling output generation based on the question and ideal answer.\n- **Few-shot Prompting** ğŸ¯: Implements few-shot prompting techniques to guide the model in generating structured feedback, using example-based input for better results.\n- **Document Understanding** ğŸ“„: Analyzes interview questions and candidate answers to generate relevant and coherent feedback based on ideal answers.\n- **Semantic Similarity Matching** ğŸ”: Employs a sentence transformer model to calculate the semantic similarity between the candidateâ€™s answer and ideal answers for more accurate feedback.\n- **Embeddings** ğŸ§ : Converts the candidateâ€™s answer and ideal response into embeddings for semantic comparison using vector space representations.\n- **Controlled Generation** ğŸ›ï¸: Generates feedback with structured suggestions and scores, ensuring output aligns with the defined structure for improvement.\n- **Fine-tuning** ğŸ”§: Implements LoRA (Low-Rank Adaptation) to fine-tune a model for personalized interview feedback generation.\n- **GenAI Evaluation** ğŸ”: This project uses **GenAI evaluation** to assess candidate answers against ideal responses. It leverages pre-trained models like Flan-T5 for structured feedback and semantic similarity matching to evaluate the relevance and quality of answers, ensuring consistent and data-driven insights.\n","metadata":{}},{"cell_type":"markdown","source":"# ğŸ“¦ Dependencies and API Access\n\n---","metadata":{}},{"cell_type":"markdown","source":"# ğŸ› ï¸ Installation Commands\n\nTo ensure all dependencies are available when running the notebook, the following installation commands are included:\n\n---\n","metadata":{}},{"cell_type":"code","source":"# Install latest Hugging Face libraries\n!pip install -U -q transformers datasets accelerate peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ”§ Package Imports and Dependencies\n\nThe **Interview Agent** project relies on several Python libraries and frameworks.  \nHere is s a consolidated list of all the imports and dependencies used throughout the notebook:\n\n---","metadata":{}},{"cell_type":"code","source":"# Core data handling and utility libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport os\nimport re\nimport shutil\nimport torch\nfrom tqdm import tqdm\n\n# Hugging Face Transformers ecosystem\nfrom transformers import pipeline\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import Dataset\n\n# Parameter-Efficient Fine-Tuning (PEFT)\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom peft import PeftModel\n\n# Semantic similarity and embeddings\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Load and Inspect Interview Dataset\n\nWe start by loading a structured dataset of technical interview questions. This dataset will serve as the backbone for our AI-driven interview assistant.\n\n---\n### ğŸ“¦ Dataset Columns:\n- **`question_id`**: Unique identifier for each question  \n- **`question`**: The interview question text  \n- **`ideal_answer`**: Concise reference answer, used for feedback comparison  \n- **`candidate_answer`**: The user's response (used during evaluation)  \n- **`difficulty`**: Labeled as Easy, Medium, or Hard  \n- **`answer_type`**: Type of answer expected (e.g., Descriptive, Code, Conceptual)  \n- **`feedback`**: Expert-generated feedback (used for training)\n\n---\n### ğŸ¯ Purpose:\nThis dataset forms the foundation for both evaluating candidate answers and generating intelligent feedback. Weâ€™ll also inspect the difficulty distribution to understand how diverse and balanced the questions are.\n","metadata":{}},{"cell_type":"code","source":"# SECTION: Load Interview Questions CSV\n# Load the dataset (update the filename/path as needed)\ndf = pd.read_csv('/kaggle/input/software-engineering-interview-questions-dataset/Software Questions.csv',encoding='latin1')\n\n# Show unique difficulty levels\nprint(\"Unique Difficulty Levels:\", df['Difficulty'].unique())\n\n# Preview dataset\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Setup, Generate, and Refine Feedback ğŸ”§\n\n### Purpose:\nThis step implements the feedback generation system using the Flan-T5 model, which is fine-tuned for few-shot prompting. The system processes candidate responses and generates structured feedback based on different answer scenarios. \n\n### Process:\n- **Input Handling**: The function handles various answer types, such as:\n  - **Empty Responses**: If no answer is provided, it generates feedback highlighting the need for a more complete response.\n  - **Brief Responses**: For short or incomplete answers, feedback is generated to prompt the candidate to elaborate on key points.\n  - **Detailed Responses**: For longer, more detailed answers, feedback is provided on the clarity, structure, and depth of the response.\n\n- **Feedback Components**:\n  - **Strengths** ğŸ’ª: Identifies positive aspects of the response.\n  - **Weaknesses** âš ï¸: Points out areas for improvement.\n  - **Suggestions** ğŸ’¡: Offers actionable suggestions to enhance the response.\n  - **Score** ğŸ†: Provides a numerical score or a rating based on predefined criteria.\n\n\n---\n\nThis step leverages the power of few-shot learning and semantic analysis to ensure that feedback is both accurate and tailored to the candidate's unique responses. It helps users understand where they excel and where they need to improve, leading to more effective preparation.\n","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/software-engineering-interview-questions-dataset/Software Questions.csv', encoding='latin1')\n\n# Load a larger model for better generation quality\nfeedback_model = pipeline(\n    \"text2text-generation\",\n    model=\"google/flan-t5-large\",  # More capable than t5-small\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9\n)\n\n# Improved Prompt with clear feedback and edge case handling\nfew_shot_prompt = \"\"\"\nYou are an AI interview coach. Your task is to evaluate a candidate's technical interview answer.\n\nProvide the feedback in the following sections:\n1. âœ… Strengths: List the strong points of the candidate's answer.\n2. âŒ Weaknesses or Misconceptions: Identify areas where the answer is lacking or incorrect.\n3. ğŸ“ˆ Suggestions for Improvement: Provide specific suggestions on how the candidate can improve their answer.\n4. ğŸ§  Score: Give a score out of 10 based on the overall quality of the answer.\n\nMake sure to fill each section with detailed and constructive feedback based on the candidate's answer.\n\nIf the candidate says \"I don't know\" or provides a vague or incomplete answer, kindly provide the correct answer or suggest areas where they can improve. Encourage learning and provide helpful feedback.\n\nExample 1:\nQuestion: What is the difference between compilation and interpretation?\nIdeal Answer: Compilation translates source code into machine code creating an executable file. Interpretation translates and executes code line by line without an executable.\nCandidate Answer: Compilation turns source code to machine code; interpretation runs code line by line.\nFeedback:\nâœ… Strengths: The candidate correctly identified the key difference between compile-time and interpretation.\nâŒ Weaknesses or Misconceptions: The candidate missed that compilation creates an executable file.\nğŸ“ˆ Suggestions for Improvement: The candidate could mention real-world examples like C (compiled) vs Python (interpreted).\nğŸ§  Score: 7/10\n\nExample 2:\nQuestion: What is polymorphism?\nIdeal Answer: Polymorphism allows methods to have different behaviors based on the object calling them.\nCandidate Answer: It allows a function to behave differently.\nFeedback:\nâœ… Strengths: The candidate has a basic understanding of polymorphism.\nâŒ Weaknesses or Misconceptions: The candidate's answer is vague and does not mention key aspects such as objects, overriding, or overloading.\nğŸ“ˆ Suggestions for Improvement: The candidate could provide examples from object-oriented programming or use analogies to clarify the concept.\nğŸ§  Score: 6/10\n\nNow evaluate this:\n\"\"\"\n\n# Function to generate feedback using the new, clear prompt with edge case handling\ndef generate_feedback(question, candidate_answer, ideal_answer):\n    # Edge case handling: Check if the answer is empty or something like \"I don't know\"\n    if candidate_answer.strip().lower() in ['i don\\'t know', 'na', '', 'no idea', 'dont know']:\n        feedback = f\"\"\"\nâœ… Strengths: The candidate was honest in stating that they don't know the answer. This is an important trait in interviews.\nâŒ Weaknesses or Misconceptions: The candidate did not attempt to answer the question.\nğŸ“ˆ Suggestions for Improvement: It would be helpful to study the key concepts related to this topic. Here's the correct answer:\n{ideal_answer}\nğŸ§  Score: 3/10 (No answer provided, but this can be improved with more preparation.)\n\"\"\"\n    # If the answer is valid but incomplete, provide suggestions to expand it\n    elif len(candidate_answer.split()) < 10:\n        feedback = f\"\"\"\nâœ… Strengths: The candidate has made an attempt to answer the question.\nâŒ Weaknesses or Misconceptions: The answer lacks detail and does not fully address the question.\nğŸ“ˆ Suggestions for Improvement: Please elaborate more on the key concepts and provide additional examples or explanations.\nğŸ§  Score: 5/10 (The answer is too brief, but it could be improved with more information.)\n\"\"\"\n    else:\n        # Generate feedback using the model\n        prompt = f\"\"\"{few_shot_prompt}\nQuestion: {question}\nIdeal Answer: {ideal_answer}\nCandidate Answer: {candidate_answer}\nFeedback:\"\"\"\n        result = feedback_model(prompt, max_length=512, truncation=True)\n        feedback = result[0]['generated_text'].strip()\n\n    return feedback\n# Live Q&A loop\nwhile True:\n    random_row = df.sample(n=1).iloc[0]\n    question = random_row['Question']\n    ideal_answer = random_row['Answer']\n\n    print(\"\\nğŸ§  Random Interview Question:\")\n    print(question)\n\n    candidate_answer = input(\"\\nğŸ‘¤ Your Answer (type 'stop' to quit): \")\n    if candidate_answer.strip().lower() == 'stop':\n        print(\"ğŸ›‘ Session ended.\")\n        break\n\n    print(\"\\nâœ… AI Feedback:\")\n    feedback = generate_feedback(question, candidate_answer, ideal_answer)\n    print(feedback)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Creating a Synthetic Dataset for Model Fine-tuning ğŸ§‘â€ğŸ’»\n\n### Purpose:\nGenerates a synthetic dataset by creating diverse candidate answers for each interview question. This dataset is crucial for fine-tuning the feedback model, ensuring it handles various response types.\n\n---\n### Process:\n- **\"I Donâ€™t Know\" Variations**: Adds multiple \"I don't know\" responses (e.g., \"I'm not sure about this one\") to simulate real candidate uncertainty.\n- **Synthetic Answer Generation**: For each question, the system generates:\n  - A random \"I donâ€™t know\" answer.\n  - Several **realistic answers** using a pre-trained model, providing diverse, humanlike responses.\n\n---\n### Dataset Structure:\nEach generated answer is stored with:\n- **question_id**, **question**, **candidate_answer**, **ideal_answer**, **difficulty**, and **answer_type** (\"unknown\" or \"humanlike\").\n\nThis synthetic dataset provides varied training examples, enhancing the model's ability to generate accurate feedback for diverse responses.\n\n---","metadata":{}},{"cell_type":"code","source":"# Load your dataset\ndf = pd.read_csv(\"/kaggle/input/software-engineering-interview-questions-dataset/Software Questions.csv\",encoding='latin1')\n\n# Load Flan-T5 large model\ngenerator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", max_length=200)\n\n# Create diverse \"I don't know\" variations\ndont_know_variants = [\n    \"I'm not sure about this one.\",\n    \"I don't know the answer to this.\",\n    \"Sorry, I can't recall this.\",\n    \"I'm unsure, but I'd like to learn more about it.\",\n    \"I don't know.\"\n]\n\n# Prepare your data\nanswer_data = []\n\ndef build_prompt(question):\n    return f\"You are a job candidate answering this interview question naturally and honestly:\\nQuestion: {question}\\nAnswer:\"\n\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    qid = row['Question Number']\n    question = row['Question']\n    ideal = row['Answer']\n    difficulty = row['Difficulty']\n\n    # Add one \"I don't know\" variation\n    answer_data.append({\n        \"question_id\": qid,\n        \"question\": question,\n        \"candidate_answer\": random.choice(dont_know_variants),\n        \"ideal_answer\": ideal,\n        \"difficulty\": difficulty,\n        \"answer_type\": \"unknown\"\n    })\n\n    # Generate 4 realistic answers\n    prompt = build_prompt(question)\n    responses = generator(prompt, num_return_sequences=4, do_sample=True, top_p=0.9, temperature=0.7)\n\n    for response in responses:\n        answer_data.append({\n            \"question_id\": qid,\n            \"question\": question,\n            \"candidate_answer\": response['generated_text'].strip(),\n            \"ideal_answer\": ideal,\n            \"difficulty\": difficulty,\n            \"answer_type\": \"humanlike\"\n        })\n\n# Convert to DataFrame\ngen_df = pd.DataFrame(answer_data)\n\n# Save to CSV\ngen_df.to_csv(\"/kaggle/working/finetune_feedback_dataset.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Generating Feedback for the Training Dataset ğŸ§ \n\n### Purpose:\nEnhances the synthetic dataset by generating feedback for each question-answer pair, forming complete training triplets: **question**, **candidate answer**, and **model-generated feedback**.\n\n---\n### Process:\n- Iterates through each row in the dataset.\n- For every candidate answer:\n  - Applies the `generate_feedback()` function using the **question**, **candidate_answer**, and **ideal_answer**.\n  - Appends the structured feedback (including strengths, weaknesses, suggestions, and score) to a new list.\n- Adds the feedback as a new column in the dataset.\n- Saves the enriched dataset as a CSV file, ready for fine-tuning the model.\n\nThis step transforms the synthetic dataset into a supervised format, enabling the model to learn how to generate high-quality feedback from real examples.\n\n---","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv('/kaggle/input/finetune/finetune_feedback_dataset.csv',encoding='latin1')\n\n# Load a capable model for feedback generation\nfeedback_model = pipeline(\n    \"text2text-generation\",\n    model=\"google/flan-t5-large\",\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9\n)\n\n# Few-shot feedback prompt template\nfew_shot_prompt = \"\"\"\nYou are an AI interview coach. Your task is to evaluate a candidate's technical interview answer.\n\nProvide the feedback in the following sections:\n1. âœ… Strengths: List the strong points of the candidate's answer.\n2. âŒ Weaknesses or Misconceptions: Identify areas where the answer is lacking or incorrect.\n3. ğŸ“ˆ Suggestions for Improvement: Provide specific suggestions on how the candidate can improve their answer.\n4. ğŸ§  Score: Give a score out of 10 based on the overall quality of the answer.\n\nMake sure to fill each section with detailed and constructive feedback based on the candidate's answer.\n\nIf the candidate says \"I don't know\" or provides a vague or incomplete answer, kindly provide the correct answer or suggest areas where they can improve. Encourage learning and provide helpful feedback.\n\nExample 1:\nQuestion: What is the difference between compilation and interpretation?\nIdeal Answer: Compilation translates source code into machine code creating an executable file. Interpretation translates and executes code line by line without an executable.\nCandidate Answer: Compilation turns source code to machine code; interpretation runs code line by line.\nFeedback:\nâœ… Strengths: The candidate correctly identified the key difference between compile-time and interpretation.\nâŒ Weaknesses or Misconceptions: The candidate missed that compilation creates an executable file.\nğŸ“ˆ Suggestions for Improvement: The candidate could mention real-world examples like C (compiled) vs Python (interpreted).\nğŸ§  Score: 7/10\n\nExample 2:\nQuestion: What is polymorphism?\nIdeal Answer: Polymorphism allows methods to have different behaviors based on the object calling them.\nCandidate Answer: It allows a function to behave differently.\nFeedback:\nâœ… Strengths: The candidate has a basic understanding of polymorphism.\nâŒ Weaknesses or Misconceptions: The candidate's answer is vague and does not mention key aspects such as objects, overriding, or overloading.\nğŸ“ˆ Suggestions for Improvement: The candidate could provide examples from object-oriented programming or use analogies to clarify the concept.\nğŸ§  Score: 6/10\n\nNow evaluate this:\n\"\"\"\n\n# Function to generate feedback\ndef generate_feedback(question, candidate_answer, ideal_answer):\n    # Ensure candidate_answer is a string and normalize it\n    candidate_answer = str(candidate_answer).strip().lower()\n    \n    # Edge case: Empty or unknown response\n    if candidate_answer in ['i don\\'t know', 'na', '', 'no idea', 'dont know']:\n        feedback = f\"\"\"\nâœ… Strengths: The candidate was honest in stating that they don't know the answer. This is an important trait in interviews.\nâŒ Weaknesses or Misconceptions: The candidate did not attempt to answer the question.\nğŸ“ˆ Suggestions for Improvement: It would be helpful to study the key concepts related to this topic. Here's the correct answer:\n{ideal_answer}\nğŸ§  Score: 3/10 (No answer provided, but this can be improved with more preparation.)\n\"\"\"\n    elif len(candidate_answer.split()) < 10:\n        feedback = f\"\"\"\nâœ… Strengths: The candidate has made an attempt to answer the question.\nâŒ Weaknesses or Misconceptions: The answer lacks detail and does not fully address the question.\nğŸ“ˆ Suggestions for Improvement: Please elaborate more on the key concepts and provide additional examples or explanations.\nğŸ§  Score: 5/10 (The answer is too brief, but it could be improved with more information.)\n\"\"\"\n    else:\n        # Use original answer casing for prompt\n        original_answer = str(candidate_answer)\n        prompt = f\"\"\"{few_shot_prompt}\nQuestion: {question}\nIdeal Answer: {ideal_answer}\nCandidate Answer: {original_answer}\nFeedback:\"\"\"\n        result = feedback_model(prompt, max_length=512, truncation=True)\n        feedback = result[0]['generated_text'].strip()\n\n    return feedback\n\n\n# Generate feedback for all rows and store in list\nfeedbacks = []\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    question = row['question']\n    candidate_answer = row['candidate_answer']\n    ideal_answer = row['ideal_answer']\n    \n    feedback = generate_feedback(question, candidate_answer, ideal_answer)\n    feedbacks.append(feedback)\n\n# Add feedback column\ndf['feedback'] = feedbacks\n\n# Save final dataset with feedback column\ndf.to_csv('/kaggle/working/finetune_feedback_dataset_with_feedback.csv', index=False)\nprint(\"âœ… Feedback generation complete! Saved to: finetune_feedback_dataset_with_feedback.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Fine-tuning the T5 Model with LoRA ğŸ§ª\n\n### Purpose:\nApplies **LoRA-based fine-tuning** to adapt the Flan-T5 model for generating structured interview feedback, using minimal compute.\n\n---\n### Process:\n- **Data Prep**: Loads and formats the dataset into input-output pairs (question, candidate answer, ideal answer â†’ feedback).\n- **Tokenization**: Converts text into model-ready tokens using the T5 tokenizer.\n- **LoRA Setup**: Applies Low-Rank Adaptation to specific layers (`q`, `v`) of the base model for efficient tuning.\n- **Training**: Trains using HuggingFaceâ€™s `Trainer` with lightweight settings (small batch, mixed precision).\n- **Save Adapters**: Saves only the LoRA adapter weights for compact deployment.\n\nThis step makes the model feedback-aware without needing to retrain the full T5 architecture. âœ…\n\n---","metadata":{}},{"cell_type":"code","source":"# === Load and clean the dataset ===\ndf = pd.read_csv(\"/kaggle/input/finetune/finetune_feedback_dataset_with_feedback.csv\")\ndf = df.dropna(subset=['feedback'])\ndf = df[df['feedback'].str.strip() != '']\n\n# Format for T5\ndf['input_text'] = df.apply(\n    lambda row: f\"Give feedback on this answer:\\nQuestion: {row['question']}\\nCandidate Answer: {row['candidate_answer']}\\nIdeal Answer: {row['ideal_answer']}\",\n    axis=1\n)\ndf['target_text'] = df['feedback']\n\n# Load as HuggingFace Dataset\ndataset = Dataset.from_pandas(df[['input_text', 'target_text']])\ndataset = dataset.train_test_split(test_size=0.1)\n\n# Tokenizer\nmodel_name = \"google/flan-t5-large\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\ndef tokenize(example):\n    input_enc = tokenizer(example['input_text'], truncation=True, padding='max_length', max_length=512)\n    target_enc = tokenizer(example['target_text'], truncation=True, padding='max_length', max_length=256)\n    input_enc[\"labels\"] = target_enc[\"input_ids\"]\n    return input_enc\n\ntokenized = dataset.map(tokenize, batched=True)\n\n# Load base model\nbase_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# === LoRA Configuration ===\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\n# Apply LoRA\nmodel = get_peft_model(base_model, lora_config)\n\n# === Training Arguments ===\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/lora-t5\",  \n    per_device_train_batch_size=2,  \n    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n    num_train_epochs=1,\n    logging_strategy=\"no\",  \n    save_strategy=\"no\",     \n    report_to=\"none\",       \n    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    disable_tqdm=False      \n)\n\n# === Trainer Setup ===\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"test\"],\n    tokenizer=tokenizer,\n)\n\n# === Train ===\ntrainer.train()\n\n# === Save LoRA Adapters ===\nmodel.save_pretrained(\"/kaggle/working/lora-t5-adapter\")\ntokenizer.save_pretrained(\"/kaggle/working/lora-t5-adapter\")\n\nprint(\"âœ… Training complete! Adapters saved in /kaggle/working/lora-t5-adapter\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Loading the Fine-tuned Model for Inference ğŸ“¥\n\n### Purpose:\nLoads the base **Flan-T5 model** along with the fine-tuned **LoRA adapter** for inference.\n\n---\n### Process:\n- Loads tokenizer and base model (`google/flan-t5-large`).\n- Integrates the trained **LoRA adapter** to restore fine-tuned weights.\n- Moves the model to GPU (if available) for faster performance.\n- Prepares the model for evaluation mode.\n\nReady for generating feedback on new candidate answers! ğŸš€\n\n---","metadata":{}},{"cell_type":"code","source":"# Paths\nbase_model_name = \"google/flan-t5-large\"\nadapter_path = \"/kaggle/input/lora-adapter-checkpoint\"  # your extracted LoRA files\n\n# Load tokenizer and base model\ntokenizer = T5Tokenizer.from_pretrained(base_model_name)\nbase_model = T5ForConditionalGeneration.from_pretrained(base_model_name)\n\n# Load the fine-tuned LoRA adapter\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nmodel.eval()\n\nprint(f\"âœ… Model loaded on {device}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Fine-tuning with LoRA ğŸ› ï¸\n\n### Purpose:\nImplements Parameter-Efficient Fine-Tuning (PEFT) using **LoRA (Low-Rank Adaptation)** to adapt the T5 model for generating high-quality interview feedback. This approach enables fine-tuning with minimal computational overhead.\n\n---\n\n### Process:\n- **Semantic Layer** ğŸ”:\n  - Uses a SentenceTransformer (`all-MiniLM-L6-v2`) to compute semantic similarity between the **candidate answer** and **key phrases** extracted from the **ideal answer**.\n  - Phrases are matched or marked as missing based on a cosine similarity threshold.\n\n- **Hybrid Feedback Generation** âš¡:\n  - Combines semantic comparison results (matched and missing phrases) with suggestions generated by the LoRA fine-tuned T5 model.\n  - The prompt includes question context, the ideal answer, and the candidateâ€™s response.\n  - The model outputs **suggestions** and **scores**, which are merged with semantic results to form a complete feedback structure:\n    - âœ… **Strengths**: Matched concepts\n    - âŒ **Weaknesses**: Missing or unclear concepts\n    - ğŸ“ˆ **Suggestions**: Model-generated improvements\n\n---\n\n### Result:\nThe feedback is detailed, accurate, and tailoredâ€”leveraging both semantic alignment and generative capabilities. This hybrid approach boosts the model's performance without requiring full-scale fine-tuning.\n\n---\n\n## Step 8: Interactive Evaluation with Hybrid Feedback Loop ğŸ¤–ğŸ’¬\n\n### Purpose:\nSimulates a **live interview scenario**, allowing users to enter responses and receive **real-time AI feedback** using the trained model.\n\n---\n\n### Process:\n- Randomly selects an interview question and corresponding ideal answer.\n- Accepts user input as the candidateâ€™s answer.\n- Checks for uncertainty phrases like â€œI donâ€™t know.â€\n- Uses **semantic similarity** to identify matched/missing concepts.\n- Generates personalized suggestions using the **fine-tuned LoRA model**.\n- Filters out contradictions and redundant advice for clean feedback.\n\n---\n\n### Result:\nDelivers actionable and structured feedback in real time, making it a powerful tool for **interview practice** and **self-assessment**. ğŸ¯\n\n---\n","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/software-engineering-interview-questions-dataset/Software Questions.csv\", encoding='latin1')\n\n# Load semantic similarity model\nsemantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Extract key phrases from the ideal answer\ndef extract_key_phrases(text):\n    return [p.strip() for p in re.split(r'[.;\\-â€¢\\n]', text) if len(p.strip()) > 5]\n\n# Match phrases from ideal to candidate based on semantic similarity\ndef get_semantic_matches(ideal_phrases, candidate_answer, threshold=0.6):\n    matches, misses = [], []\n    answer_embedding = semantic_model.encode(candidate_answer, convert_to_tensor=True,show_progress_bar=False)\n\n    for phrase in ideal_phrases:\n        phrase_embedding = semantic_model.encode(phrase, convert_to_tensor=True,show_progress_bar=False)\n        sim = util.pytorch_cos_sim(phrase_embedding, answer_embedding).item()\n        if sim > threshold:\n            matches.append(phrase)\n        else:\n            misses.append(phrase)\n    return matches, misses\n\n# Optional: Check for \"I don't know\"-type answers\ndef is_uncertain_answer(text):\n    uncertain_keywords = [\"i don't know\", \"not sure\", \"no idea\", \"can't say\"]\n    return any(kw in text.lower() for kw in uncertain_keywords)\n    \nfew_shot_prompt = \"\"\"\nBelow are examples of interview questions, ideal answers, candidate answers, and helpful feedback focusing only on suggestions for improvement and a score.\n\nExample 1:\nQuestion: What is the difference between an abstract class and an interface in Java?\nIdeal Answer: An abstract class can have method implementations and member variables, while an interface can only have method declarations (before Java 8). A class can extend only one abstract class but implement multiple interfaces.\nCandidate Answer: Abstract classes and interfaces are almost the same. Both can have abstract methods.\nFeedback (only generate suggestion and score):\nğŸ“ˆ Suggestions for Improvement: The candidate should mention that interfaces allow multiple inheritance and abstract classes allow implemented methods. Also, Java 8+ allows default methods in interfaces.\nğŸ§  Score: 5/10\n\nExample 2:\nQuestion: What is normalization in databases?\nIdeal Answer: Normalization is the process of organizing data to reduce redundancy and improve integrity by splitting data into related tables and defining relationships.\nCandidate Answer: Itâ€™s about organizing a database.\nFeedback (only generate suggestion and score):\nğŸ“ˆ Suggestions for Improvement: The candidate should explain how normalization removes redundancy and mention normal forms (1NF, 2NF, etc.).\nğŸ§  Score: 4/10\n\"\"\"\n\n# Feedback generation with hybrid semantic + fine-tuned model\ndef generate_feedback(question, candidate_answer, ideal_answer):\n    if is_uncertain_answer(candidate_answer):\n        return f\"\"\"\nâœ… \\033[92mStrengths:\\033[0m It's good that the candidate acknowledged their uncertainty.\nâŒ \\033[91mWeaknesses or Misconceptions:\\033[0m No relevant content was provided.\nğŸ“ˆ \\033[94mSuggestions for Improvement:\\033[0m Here's a good explanation:\n{ideal_answer}\nğŸ§  \\033[95mScore:\\033[0m 2/10 (With more study, you can master this!)\n\"\"\"\n\n    # Semantic comparison\n    ideal_phrases = extract_key_phrases(ideal_answer)\n    matched, missing = get_semantic_matches(ideal_phrases, candidate_answer)\n\n    # LLM suggestion only\n    prompt = f\"\"\"{few_shot_prompt}\nQuestion: {question}\nIdeal Answer: {ideal_answer}\nCandidate Answer: {candidate_answer}\nFeedback (only generate suggestion and score):\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n    outputs = model.generate(**inputs, max_length=512)\n    suggestion_output = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n\n    # Filter contradictions and repetitions from suggestions\n    cleaned_lines = []\n    for line in suggestion_output.splitlines():\n        if any(kw in line.lower() for kw in [\"good understanding\", \"poor understanding\", \"weaknesses\"]):\n            continue  # Remove contradiction\n        if any(match.lower() in line.lower() for match in matched):\n            continue  # Remove redundancy\n        cleaned_lines.append(line.strip())\n\n    final_suggestion = \"\\n\".join(cleaned_lines).strip()\n\n    # Format semantic feedback\n    matched_str = ', '.join(matched) if matched else 'None'\n    missing_str = ', '.join(missing) if missing else 'None'\n\n    return f\"\"\"\nâœ… \\033[92mStrengths:\\033[0m Covered key ideas: {matched_str}\nâŒ \\033[91mWeaknesses or Misconceptions:\\033[0m Missing or unclear concepts: {missing_str}\nğŸ“ˆ \\033[94mSuggestions for Improvement:\\033[0m\n{final_suggestion if final_suggestion else suggestion_output}\n\"\"\"\n\n# ========== Live Q&A Loop ==========\nwhile True:\n    random_row = df.sample(n=1).iloc[0]\n    question = random_row['Question']\n    ideal_answer = random_row['Answer']\n\n    print(\"\\nğŸ§  Random Interview Question:\")\n    print(question)\n\n    candidate_answer = input(\"\\nğŸ‘¤ Your Answer (type 'stop' to quit): \")\n    if candidate_answer.strip().lower() == 'stop':\n        print(\"ğŸ›‘ Session ended.\")\n        break\n\n    print(\"\\nâœ… AI Feedback:\")\n    feedback = generate_feedback(question, candidate_answer, ideal_answer)\n    print(feedback)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## âœ… Conclusion\n\nThis project introduces a hybrid feedback generation system that combines:\n\n- **Semantic similarity matching** using `SentenceTransformer` to identify strengths and weaknesses in candidate responses.\n- **LoRA fine-tuned T5 model** to generate constructive suggestions and score answers.\n\nBy blending semantic understanding with generative feedback, the system delivers high-quality, tailored feedback that supports learners in improving their interview performance. This parameter-efficient approach enables effective adaptation without extensive computational resources.\n\nThe method demonstrates strong potential for scalable, automated feedback in technical learning environments.\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"## ğŸ‘¤ Collaborators\n\nThis project was created by **Laksh Jain** as part of a solo initiative exploring intelligent interview feedback systems using semantic similarity and fine-tuned models.\n","metadata":{}}]}